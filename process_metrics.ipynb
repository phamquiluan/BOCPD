{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Changepoint Detection with multivariate data in Python\n",
    "\n",
    "This code computes the probability of changepoints (including changes in correlation) in a time series. In this notebook I show how you can use it. This example is modified from Xiang Xuan's thesis Section 3.2.\n",
    "\n",
    "First let's generate some data and load some modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T16:06:14.135806Z",
     "start_time": "2021-11-26T16:06:13.243942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use scipy logsumexp().\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import bayesian_changepoint_detection.generate_data as gd\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from os.path import basename,join,dirname\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "from tqdm import tqdm\n",
    "from bayesian_changepoint_detection.priors import const_prior\n",
    "from bayesian_changepoint_detection.offline_likelihoods import IndepentFeaturesLikelihood\n",
    "import bayesian_changepoint_detection.online_likelihoods as online_ll\n",
    "from bayesian_changepoint_detection.bayesian_models import offline_changepoint_detection \n",
    "from bayesian_changepoint_detection.bayesian_models import online_changepoint_detection\n",
    "from functools import partial\n",
    "from bayesian_changepoint_detection.hazard_functions import constant_hazard\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def find_anomalies(data, threshold=0.01):\n",
    "    anomalies = []\n",
    "    for i in range(1, len(data)):\n",
    "        if data[i] > threshold:\n",
    "            anomalies.append(i)\n",
    "\n",
    "    # re-try if threshold doesn't work\n",
    "    if len(anomalies) == 0:\n",
    "        head = 5\n",
    "        data = data[head:]\n",
    "        anomalies = [np.argmax(data) + head]\n",
    "\n",
    "    # merge continuous anomalies if the distance are shorter than 5 steps\n",
    "    merged_anomalies = [] if len(anomalies) == 0 else [anomalies[0]]\n",
    "    for i in range(1, len(anomalies)):\n",
    "        if anomalies[i] - anomalies[i-1] > 5:\n",
    "            merged_anomalies.append(anomalies[i])\n",
    "    \n",
    "    return merged_anomalies, anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T16:07:28.664794Z",
     "start_time": "2021-11-26T16:07:27.889503Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:35<00:00,  2.84it/s]\n"
     ]
    }
   ],
   "source": [
    "sparsity = 5 \n",
    "epsilon = 1e-7\n",
    "\n",
    "for data_path in tqdm(glob.glob(\"../cfm/data/fse-ss/**/simple_data.csv\", recursive=True)):\n",
    "    service_metric = basename(dirname(dirname(data_path)))\n",
    "    case_idx = basename(dirname(data_path))\n",
    "    \n",
    "    # if exist, ignore\n",
    "    if os.path.exists(f\"./fse-ss/{service_metric}_{case_idx}.json\"):\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    data = pd.read_csv(data_path)   \n",
    "    selected_cols = [c for c in data.columns if \"latency-50\" in c and 'queue-master' not in c]\n",
    "    data = data[selected_cols]\n",
    "    data = data.fillna(method=\"ffill\")\n",
    "    data = data.fillna(0)\n",
    "\n",
    "    for c in data.columns:\n",
    "        data[c] = (data[c] - np.min(data[c])) / (np.max(data[c]) - np.min(data[c]))\n",
    "    data = data.fillna(method=\"ffill\")\n",
    "    data = data.fillna(0)\n",
    "    \n",
    "    data = data.to_numpy()\n",
    "\n",
    "    # print(data)\n",
    "    # run \n",
    "    start = datetime.now()\n",
    "    R, maxes = online_changepoint_detection(\n",
    "            data,\n",
    "            partial(constant_hazard, 50),\n",
    "            online_ll.MultivariateT(dims=data.shape[1])\n",
    "    )\n",
    "    time_taken = datetime.now() - start\n",
    "    with open(f\"./fse-ss/{service_metric}_{case_idx}.txt\", \"w\") as f:\n",
    "        f.write(f\"Time taken: {time_taken}\")\n",
    "\n",
    "    #plot \n",
    "\n",
    "    density_matrix = -np.log(R[0:-1:sparsity, 0:-1:sparsity]+epsilon)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(3, figsize=[8, 8])\n",
    "    for d in range(data.shape[1]):\n",
    "        ax[0].plot(data[:,d])\n",
    "    ax[1].pcolor(\n",
    "        np.array(range(0, len(R[:,0]), sparsity)), \n",
    "        np.array(range(0, len(R[:,0]), sparsity)), \n",
    "        density_matrix, \n",
    "        cmap=cm.Greys, vmin=0, vmax=density_matrix.max(),\n",
    "        shading='auto'\n",
    "    )\n",
    "    Nw=10\n",
    "    out = R[Nw,Nw:-1]\n",
    "    for i in range(50): # can't accept the first 50 elements\n",
    "        out[i] = 0\n",
    "    # ax[2].plot(R[Nw,Nw:-1])\n",
    "    ax[2].plot(out)\n",
    "    plt.legend(['Raw data with Original Changepoints'])\n",
    "    # plt.show()\n",
    "    plt.savefig(f\"./fse-ss/{service_metric}_{case_idx}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    out = out.tolist()\n",
    "    # write to file\n",
    "    with open(f\"./fse-ss/{service_metric}_{case_idx}.json\", \"w\") as f:\n",
    "        json.dump(out, f)\n",
    "        \n",
    "    anomalies = find_anomalies(out)[0]\n",
    "    anomalies = list(map(int, anomalies))\n",
    "    # write to file\n",
    "    with open(f\"./fse-ss/{service_metric}_{case_idx}_anomalies.json\", \"w\") as f:\n",
    "        json.dump(anomalies, f)\n",
    "    # break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annorxiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
